{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53cae3e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1908\\4157498325.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "import selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76013f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.9.1-py3-none-any.whl (6.6 MB)\n",
      "     ---------------------------------------- 6.6/6.6 MB 3.8 MB/s eta 0:00:00\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.10.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\anjal sharma\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
      "     -------------------------------------- 384.9/384.9 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\anjal sharma\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\anjal sharma\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\anjal sharma\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\anjal sharma\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\anjal sharma\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\anjal sharma\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Collecting async-generator>=1.9\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\anjal sharma\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\anjal sharma\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     -------------------------------------- 58.3/58.3 kB 774.5 kB/s eta 0:00:00\n",
      "Installing collected packages: outcome, h11, exceptiongroup, async-generator, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed async-generator-1.10 exceptiongroup-1.1.1 h11-0.14.0 outcome-1.2.0 selenium-4.9.1 trio-0.22.0 trio-websocket-0.10.2 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f4bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8cbd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Anjal\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87ca66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.naukri.com\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3b5c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_job = driver.find_element_by_xpath(\"//input[@class='sugInp']\")\n",
    "search_job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225896d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on search bar\n",
    "search_job.send_keys('Data Analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab25af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job location bar\n",
    "search_loc=driver.find_element_by_id('qsb-location-sugg')\n",
    "search_loc.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02629f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn= driver .find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "063f4b1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10872\\3479787028.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# clicking  using xpath function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msearch_btn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"//button[@class='btn']\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msearch_btn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "# clicking  using xpath function\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee11cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so let's extract all the tags having the job titles\n",
    "title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "title_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ae0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the text of the job title  from the tags\n",
    "job_titles=[]\n",
    "for i in title_tags:\n",
    "    if i.text is None:\n",
    "        job_titles.append('Not')\n",
    "    else:\n",
    "        job_titles.append(i.text)\n",
    "job_titles[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e4eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets extract all the tags having company names\n",
    "company_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "company_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b6dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will extract the text from the tags by looping over these tags\n",
    "companies_names=[]\n",
    "\n",
    "for i in company_tags:\n",
    "    companies_names.append(i.text)\n",
    "companies_names[:10]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d264228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so lets extract  all the tags having the experience required data\n",
    "experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience'] //span\")\n",
    "experience_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1efbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no we will extract the text  from these tags only by one by looping over these tags\n",
    "experience_list=[]\n",
    "for i in experience_tags:\n",
    "    experience_list.append(i.text)\n",
    "experience_list[:10]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07ae71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "locations_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26009982",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_list=[]\n",
    "for i in locations_tags:\n",
    "    locations_list.append(i.text)\n",
    "locations_list[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182eb37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So lets check th length of ech element.\n",
    "print(len(job_titles[:10])),print(len(companies_names[:10])),print(len(experience_list[:10])),print(len(locations_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37eb7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs=pd.DataFrame({})\n",
    "jobs['title']=job_titles[:10]\n",
    "jobs['company']=companies_names[:10]\n",
    "jobs['experience_required']=experience_list[:10]\n",
    "jobs['location']=locations_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b77d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc49681",
   "metadata": {},
   "source": [
    "# qUESTION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e985ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Anjal\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e1158",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.naukri.com\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bf6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_job = driver.find_element_by_xpath(\"//input[@class='sugInp']\")\n",
    "search_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209603c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on search bar\n",
    "search_job.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff40519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job location bar\n",
    "search_loc=driver.find_element_by_id('qsb-location-sugg')\n",
    "search_loc.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58791ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn= driver .find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc3f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking  using xpath function\n",
    "search_btn=driver.find_element_by_class_name('btn')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee6cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so let's extract all the tags having the job titles\n",
    "title_tag=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "title_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05264cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the text of the job title  from the tags\n",
    "job1_titles=[]\n",
    "for i in title_tag:\n",
    "    if i.text is None:\n",
    "        job1_titles.append('Not')\n",
    "    else:\n",
    "        job1_titles.append(i.text)\n",
    "job1_titles[:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7242fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets extract all the tags having company names\n",
    "company_tag=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "company_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7bca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will extract the text from the tags by looping over these tags\n",
    "companies1_names=[]\n",
    "\n",
    "for i in company_tag:\n",
    "    companies1_names.append(i.text)\n",
    "companies1_names[:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29561cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets extract all the tags having locations\n",
    "locations_tag=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "locations_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations1_list=[]\n",
    "for i in locations_tag:\n",
    "    locations1_list.append(i.text)\n",
    "locations1_list[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d68285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job1_titles[:10])),print(len(companies1_names[:10])),print(len(locations1_list[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b489d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Anjal\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver.get('https://www.naukri.com/data-scientist-jobs-in-banglore-bagaluru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf25574",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece46f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b4047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\"):\n",
    "    urls.append(i.get_attribute(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22598834",
   "metadata": {},
   "outputs": [],
   "source": [
    "or url in urls[:10]:\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        description=driver.find_element_by_xpath(\"//section[@class='job-desc']\").text\n",
    "        job_description.append(description)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        job_description.append(\"Not Available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3664fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9b75fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "job1=pd.DataFrame({})\n",
    "job1['title']=job1_titles[:10]\n",
    "job1['company_name']=companies1_names[:10]\n",
    "job1['location']=locations1_list[:10]\n",
    "job1['job_desc']=job_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b03c4c",
   "metadata": {},
   "source": [
    "# question  3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Anjal\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daed268",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.naukri.com\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_job = driver.find_element_by_xpath(\"//input[@class='sugInp']\")\n",
    "search_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dfb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on search bar\n",
    "search_job.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7137c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn= driver .find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91213822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking  using xpath function\n",
    "search_btn=driver.find_element_by_class_name('btn')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabffa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so let's extract all the tags having the job titles\n",
    "title_t1=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "title_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e8eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the text of the job title  from the tags\n",
    "job_titles=[]\n",
    "for i in title_t1:\n",
    "    if i.text is None:\n",
    "        job_titles.append('Not')\n",
    "    else:\n",
    "        job_titles.append(i.text)\n",
    "job_titles[:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ccff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets extract all the tags having company names\n",
    "company_t1=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "company_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec588787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will extract the text from the tags by looping over these tags\n",
    "companies_names=[]\n",
    "\n",
    "for i in company_t1:\n",
    "    companies_names.append(i.text)\n",
    "companies_names[:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ba03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so lets extract  all the tags having the experience required data\n",
    "experience_t1=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience'] //span\")\n",
    "experience_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4108905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no we will extract the text  from these tags only by one by looping over these tags\n",
    "experience_list=[]\n",
    "for i in experience_t1:\n",
    "    experience_list.append(i.text)\n",
    "experience_list[:10]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d8fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So lets extract all the tags having locations\n",
    "locations_t1=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "locations_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55af8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we wil extract the text from these tags only by one by looping over these tags\n",
    "locations_list=[]\n",
    "for i in locations_t1:\n",
    "    locations_list.append(i.text)\n",
    "locations_list[:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6083a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So lets check th length of ech element.\n",
    "print(len(job_titles[:10])),print(len(companies_names[:10])),print(len(experience_list[:10])),print(len(locations_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3d06fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs2=pd.DataFrame({})\n",
    "jobs2['title']=job_titles[:10]\n",
    "jobs2['company']=companies_names[:10]\n",
    "jobs2['experience_required']=experience_list[:10]\n",
    "jobs2['location']=locations_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7099ce12",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c18ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Anjal\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31d64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.flipkart.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11394423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_g= driver.find_element_by_xpath(\"//input[@type='text']\")\n",
    "search_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5768a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on search bar\n",
    "search_g.send_keys('sunglasses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d34dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn=driver.find_element_by_class_name('L0Z3Pu')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23eddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_name=[]\n",
    "Price=[]\n",
    "P_desc=[]\n",
    "Discount=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f5ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    b_name=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    p_desc=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    price =driver.find_elements_by_xpath(\"//div[@class='_25b18c']\")\n",
    "    discount=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\")\n",
    "    \n",
    "    for j  in b_name:\n",
    "        B_name.append(j.text)\n",
    "    B_name[:100]    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for k in p_desc:\n",
    "        P_desc.append(k.text)\n",
    "    P_desc[:100] \n",
    "    \n",
    "    \n",
    "    for l in price:\n",
    "        Price.append(l.text)\n",
    "    Price[:100] \n",
    "    \n",
    "    \n",
    "    for t in discount:\n",
    "        Discount.append(t.text)\n",
    "    Discount[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb17da75",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_name[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426a9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(B_name[:100])),print(len(Price[:100])),print(len(P_desc[:100])),print(len(Discount[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344fe103",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gl=pd.DataFrame({})\n",
    "sun_gl['Brand_name']=B_name[:100]\n",
    "sun_gl['P_price']=Price[:100]\n",
    "sun_gl['Pr_desc']=P_desc[:100]\n",
    "sun_gl['P_discount']=Discount[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bedaf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sun_gl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46978b0",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Anjal\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301ad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.flipkart.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_g= driver.find_element_by_xpath(\"//input[@type='text']\")\n",
    "search_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c826fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on search bar\n",
    "search_g.send_keys('sneakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a8d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9675e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba238b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_name=[]\n",
    "Price=[]\n",
    "P_desc=[]\n",
    "Discount=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83729758",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    b_name=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    p_desc=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    price =driver.find_elements_by_xpath(\"//div[@class='_25b18c']\")\n",
    "    discount=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\")\n",
    "    \n",
    "    for j  in b_name:\n",
    "        B_name.append(j.text)\n",
    "    B_name[:100]    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for k in p_desc:\n",
    "        P_desc.append(k.text)\n",
    "    P_desc[:100] \n",
    "    \n",
    "    \n",
    "    for l in price:\n",
    "        Price.append(l.text)\n",
    "    Price[:100] \n",
    "    \n",
    "    \n",
    "    for t in discount:\n",
    "        Discount.append(t.text)\n",
    "    Discount[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a6ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(B_name[:100])),print(len(Price[:100])),print(len(P_desc[:100])),print(len(Discount[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5495dc3",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521910d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Anjal\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0578aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\" https://www.amazon.in \"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4ef536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_g= driver.find_element_by_xpath(\"//input[@type='text']\")\n",
    "search_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea8289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on search bar\n",
    "search_g.send_keys('Laptop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e19507",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn=driver.find_element_by_xpath(\"//input[@id='nav-search-submit-button']\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1372663",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn=driver.find_element_by_xpath(\"//input[@id='nav-search-submit-button']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b65859",
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "Price=[]\n",
    "Rating=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ae655",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    b_name=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    p_desc=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    price =driver.find_elements_by_xpath(\"//div[@class='_25b18c']\")\n",
    "    \n",
    "    \n",
    "    for j  in b_name:\n",
    "        Title.append(j.text)\n",
    "    Title[:100]    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for k in p_desc:\n",
    "        P_desc.append(k.text)\n",
    "    P_desc[:100] \n",
    "    \n",
    "    \n",
    "    for l in price:\n",
    "        Price.append(l.text)\n",
    "    Price[:100] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(b_name[:100])),print(len(Price[:100])),print(len(P_desc[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb0004",
   "metadata": {},
   "source": [
    "# Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage content\n",
    "url = 'https://www.jagranjosh.com/'\n",
    "response = requests.get(url)\n",
    "content = response.content\n",
    "\n",
    "# Step 2: Parse the HTML content\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Step 3: Find the required link and navigate to it\n",
    "gk_link = soup.find('a', text='GK')\n",
    "gk_url = gk_link['href']\n",
    "gk_response = requests.get(gk_url)\n",
    "gk_content = gk_response.content\n",
    "gk_soup = BeautifulSoup(gk_content, 'html.parser')\n",
    "\n",
    "# Step 4: Find the link to the list of Prime Ministers and navigate to it\n",
    "pm_link = gk_soup.find('a', text='List of all Prime Ministers of India')\n",
    "pm_url = pm_link['href']\n",
    "pm_response = requests.get(pm_url)\n",
    "pm_content = pm_response.content\n",
    "pm_soup = BeautifulSoup(pm_content, 'html.parser')\n",
    "\n",
    "# Step 5: Scrape the data and create a DataFrame\n",
    "rows = pm_soup.find_all('tr')\n",
    "data = []\n",
    "\n",
    "for row in rows[1:]:  # Skip the header row\n",
    "    columns = row.find_all('td')\n",
    "    name = columns[0].text.strip()\n",
    "    born_dead = columns[1].text.strip()\n",
    "    term_of_office = columns[2].text.strip()\n",
    "    remarks = columns[3].text.strip()\n",
    "    data.append([name, born_dead, term_of_office, remarks])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Name', 'Born-Dead', 'Term of Office', 'Remarks'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa78f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage content\n",
    "url = 'https://www.motor1.com/'\n",
    "response = requests.get(url)\n",
    "content = response.content\n",
    "\n",
    "# Step 2: Parse the HTML content\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Step 3: Find the search bar and enter the search term\n",
    "search_form = soup.find('form', class_='search-form')\n",
    "search_input = search_form.find('input', class_='search-input')\n",
    "search_input['value'] = '50 most expensive cars'\n",
    "\n",
    "# Step 4: Submit the search form\n",
    "submit_button = search_form.find('button', class_='search-submit')\n",
    "search_url = submit_button['formaction']\n",
    "search_response = requests.get(search_url, params={'q': '50 most expensive cars'})\n",
    "search_content = search_response.content\n",
    "search_soup = BeautifulSoup(search_content, 'html.parser')\n",
    "\n",
    "# Step 5: Find the link to the list of 50 most expensive cars and navigate to it\n",
    "result_link = search_soup.find('a', text='50 Most Expensive Cars in the World')\n",
    "result_url = result_link['href']\n",
    "result_response = requests.get(result_url)\n",
    "result_content = result_response.content\n",
    "result_soup = BeautifulSoup(result_content, 'html.parser')\n",
    "\n",
    "# Step 6: Scrape the data and create a DataFrame\n",
    "car_list = result_soup.find('ol', class_='listicle-items')\n",
    "cars = car_list.find_all('li')\n",
    "\n",
    "data = []\n",
    "for car in cars:\n",
    "    name = car.find('h3').text.strip()\n",
    "    price = car.find('span', class_='price').text.strip()\n",
    "    data.append([name, price])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Car Name', 'Price'])\n",
    "\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b472f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define the URL\n",
    "url = 'https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART'\n",
    "\n",
    "# Step 2: Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 3: Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Step 4: Find the review sections\n",
    "review_sections = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "\n",
    "reviews_data = []\n",
    "count = 0\n",
    "\n",
    "# Step 5: Loop through each review section\n",
    "for section in review_sections:\n",
    "    count += 1\n",
    "\n",
    "    # Step 6: Extract the required attributes from each review section\n",
    "    rating = section.find('div', {'class': '_3LWZlK _1BLPMq'}).text.strip()\n",
    "    summary = section.find('p', {'class': '_2-N8zT'}).text.strip()\n",
    "    full_review = section.find('div', {'class': 't-ZTKy'}).div.div.text.strip()\n",
    "\n",
    "    # Step 7: Append the data to the list\n",
    "    reviews_data.append([rating, summary, full_review])\n",
    "\n",
    "    # Step 8: Break the loop if 100 reviews have been scraped\n",
    "    if count == 100:\n",
    "        break\n",
    "\n",
    "# Step 9: Create a DataFrame with the scraped data\n",
    "df = pd.DataFrame(reviews_data, columns=['Rating', 'Review Summary', 'Full Review'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13757d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d015e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d62c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65824168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12276dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1920658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aadee8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0922d6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472fd763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af8e64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb1c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb921c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d55bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aacb441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc903c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94222341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b705aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003cb026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
